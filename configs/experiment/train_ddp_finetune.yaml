# @package _global_

# to execute this experiment run:
# python train.py experiment=train_ddp; it will collect the parameters from the default files specified below, 
# and override certain parameters as shown below for trainer and data classes

#default files used to set up the parameters; you can change the .yaml file if you want to use another file as default;
# e.g. if you want to use tensorboard instaed of wandb as a logger, please change the line 13 to tensorboard.yaml
defaults:
  - override /data: oneprot_finetune.yaml
  - override /model: oneprot_finetune.yaml
  - override /callbacks: default.yaml
  - override /logger: wandb.yaml
  - override /trainer: ddp.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["design", "facebook/esm2_t36_3B_UR50D"]

seed: 12345

# overriding certain classes; you can also override other classes (e.g. model, callbacks etc.)

trainer:
  min_epochs: 1
  max_epochs: 100
  #max_steps: 10
  num_nodes: 1
  #precision: 16
  #gradient_clip_val: 1.0
  num_sanity_val_steps: 5
  #val_check_interval: 250

data:
  data_modalities: ['design']
  seq_tokenizer: "facebook/esm2_t33_650M_UR50D"
  text_tokenizer: microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext
  batch_size: 4
