{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unimol.models import UniMolModel\n",
    "from unimol.tasks.unimol_pocket import UniMolPocketTask\n",
    "from unicore.data import Dictionary\n",
    "from unicore import checkpoint_utils\n",
    "from unicore import tasks\n",
    "from unicore.logging import progress_bar\n",
    "\n",
    "import os\n",
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "args=Namespace(no_progress_bar=False, log_interval=10, log_format='simple', tensorboard_logdir='./save_pocket//tsb',\n",
    "         wandb_project='', wandb_name='', seed=1, cpu=False, fp16=True, bf16=False, bf16_sr=False, allreduce_fp32_grad=False, \n",
    "         fp16_no_flatten_grads=False, fp16_init_scale=4, fp16_scale_window=256, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, \n",
    "         threshold_loss_scale=None, user_dir='./unimol', empty_cache_freq=0, all_gather_list_size=16384, suppress_crashes=False, \n",
    "         profile=False, ema_decay=-1.0, validate_with_ema=False, loss='unimol', optimizer='adam', lr_scheduler='polynomial_decay', \n",
    "         task='unimol_pocket', num_workers=4, skip_invalid_size_inputs_valid_test=False, batch_size=32, required_batch_size_multiple=1, \n",
    "         data_buffer_size=10, train_subset='train', valid_subset='val', validate_interval=1, validate_interval_updates=10000, \n",
    "         validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, batch_size_valid=32, max_valid_steps=None, \n",
    "         curriculum=0, distributed_world_size=8, distributed_rank=1, distributed_backend='nccl', distributed_init_method='env://', \n",
    "         distributed_port=-1, device_id=1, distributed_no_spawn=True, ddp_backend='c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, \n",
    "         find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, nprocs_per_node=4, arch='unimol_base', max_epoch=0, \n",
    "         max_update=1000000, stop_time_hours=0, clip_norm=1.0, per_sample_clip_norm=0, update_freq=[1], lr=[0.0001], stop_min_lr=-1, \n",
    "         save_dir='./save_pocket/', tmp_save_dir='./', restore_file='checkpoint_last.pt', finetune_from_model=None, load_from_ema=False, \n",
    "         reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, \n",
    "         save_interval_updates=10000, keep_interval_updates=10, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, \n",
    "         no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', \n",
    "         maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', mode='infer', data='/p/scratch/found/unimol_datasets/pockets/', \n",
    "         mask_prob=0.15, leave_unmasked_prob=0.05, random_token_prob=0.05, noise_type='uniform', noise=1.0, remove_hydrogen=False, \n",
    "         remove_polar_hydrogen=False, max_atoms=256, dict_name='dict_coarse.txt', adam_betas='(0.9, 0.99)', adam_eps=1e-06, weight_decay=0.0001, \n",
    "         force_anneal=None, warmup_updates=10000, warmup_ratio=-1.0, end_learning_rate=0.0, power=1.0, total_num_update=1000000, masked_token_loss=1.0, \n",
    "         masked_coord_loss=1.0, masked_dist_loss=1.0, x_norm_loss=0.01, delta_pair_repr_norm_loss=0.01, no_seed_provided=False, encoder_layers=15, \n",
    "         encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_attention_heads=64, dropout=0.1, emb_dropout=0.1, attention_dropout=0.1, \n",
    "         activation_dropout=0.0, pooler_dropout=0.0, max_seq_len=512, activation_fn='gelu', pooler_activation_fn='tanh', post_ln=False)\n",
    "\n",
    "dictionary = Dictionary.load(os.path.join(args.data, args.dict_name))\n",
    "        \n",
    "#args=Namespace(no_progress_bar=False, log_interval=10,log_format='simple', tensorboard_logdir='./save_pocket//tsb', wandb_project='', wandb_name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class adding some layers to Unimol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import TensorType\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "#from src.models.components.layers import LearnableLogitScaling, Normalize\n",
    "\n",
    "class PocketModel(nn.Module):\n",
    "\n",
    "    output_tokens: torch.jit.Final[bool]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            #encoder: torch.nn.Module,\n",
    "            task,\n",
    "            args,\n",
    "            dictionary,\n",
    "            ckpt_path=None,\n",
    "            output_dim: int = 512,\n",
    "            proj: str = None,\n",
    "            use_logit_scale: str = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        task=UniMolPocketTask(args, dictionary)\n",
    "        task_pocket=task.setup_task(args)\n",
    "        model=task_pocket.build_model(args)\n",
    "        \n",
    "        if ckpt_path is not None:\n",
    "            state = checkpoint_utils.load_checkpoint_to_cpu(ckpt_path)\n",
    "            model.load_state_dict(state[\"model\"], strict=False)\n",
    "        \n",
    "        self.encoder=model\n",
    "        self.output_dim = output_dim\n",
    "        d_model = output_dim\n",
    "\n",
    "        if (d_model == output_dim) and (proj is None):  # do we always need a proj?\n",
    "            self.proj = nn.Identity()\n",
    "        elif proj == 'linear':\n",
    "            self.proj = nn.Linear(d_model, output_dim, bias=False)\n",
    "        \n",
    "        # if use_logit_scale:\n",
    "        #     self.norm = nn.Sequential(\n",
    "        #                     Normalize(dim=-1), \n",
    "        #                     LearnableLogitScaling(learnable=True)\n",
    "        #             )\n",
    "        # else:\n",
    "        #     self.norm = nn.Sequential(\n",
    "        #                     Normalize(dim=-1), \n",
    "        #                     LearnableLogitScaling(learnable=False)\n",
    "        #             )\n",
    "\n",
    "    def forward(self, batch: collections.OrderedDict):\n",
    "        \n",
    "        src_tokens,src_distance,src_coord,src_edge_type = batch.values()\n",
    "        pooled_out = self.encoder(src_tokens,src_coord,src_distance,src_edge_type)[0]\n",
    "        projected = self.proj(pooled_out)\n",
    "        #normed = self.norm(projected) \n",
    "        #return normed\n",
    "        return projected\n",
    "\n",
    "    def lock(self, unlocked_layers: int = 0, freeze_layer_norm: bool = True):\n",
    "        if not unlocked_layers:  # full freezing\n",
    "            for n, p in self.model.named_parameters():\n",
    "                p.requires_grad = (not freeze_layer_norm) if \"LayerNorm\" in n.split(\".\") else False\n",
    "            return\n",
    "\n",
    "    def init_parameters(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the model to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of init!!!!\n"
     ]
    }
   ],
   "source": [
    "model=PocketModel(UniMolPocketTask,args,dictionary,'save_pocket/checkpoint37.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['net_input.src_tokens', 'net_input.src_coord', 'net_input.src_distance', 'net_input.src_edge_type', 'target.tokens_target', 'target.distance_target', 'target.coord_target', 'target.pdb_id'])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset=args.valid_subset.split(\",\")[0]\n",
    "task=UniMolPocketTask(args, dictionary)\n",
    "task.load_dataset(subset, combine=False, epoch=1)\n",
    "dataset = task.dataset(subset)\n",
    "dataset[0].keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "building the batch iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parallel_world_size = 1\n",
    "data_parallel_rank = 0\n",
    "\n",
    "itr = task.get_batch_iterator(\n",
    "            dataset=dataset,\n",
    "            batch_size=args.batch_size,\n",
    "            ignore_invalid_inputs=True,\n",
    "            required_batch_size_multiple=args.required_batch_size_multiple,\n",
    "            seed=args.seed,\n",
    "            num_shards=data_parallel_world_size,\n",
    "            shard_id=data_parallel_rank,\n",
    "            num_workers=args.num_workers,\n",
    "            data_buffer_size=args.data_buffer_size,\n",
    "        ).next_epoch_itr(shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P06766'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# src_tokens,src_distance,src_coord,src_edge_type= next(itr)['net_input'].values()\n",
    "# src_tokens.shape, src_distance.shape, src_coord.shape, src_edge_type.shape\n",
    "# next(itr)['target']['pdb_id'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "passing an input to a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6868, -0.5002,  0.0629,  ...,  0.2643,  0.6296,  0.4229],\n",
       "         [ 0.0705, -0.9110, -0.4445,  ..., -0.0536,  0.0799, -0.5051],\n",
       "         [-1.5933, -0.6398, -0.5973,  ...,  0.5244, -0.7628, -1.6311],\n",
       "         ...,\n",
       "         [ 0.6576, -0.9981, -1.3797,  ..., -0.6243, -0.2075, -0.3160],\n",
       "         [ 1.5873, -1.6611, -1.5267,  ..., -1.6872,  0.3415,  0.9952],\n",
       "         [ 0.5869, -0.7048, -1.2732,  ..., -1.7372, -0.4952, -1.1116]],\n",
       "\n",
       "        [[ 0.6649, -1.2847,  0.0309,  ...,  0.3215,  1.3422,  0.2808],\n",
       "         [-0.8561, -0.6664, -0.6936,  ..., -1.7930,  1.3666, -0.1981],\n",
       "         [-0.7612, -0.7434, -0.2390,  ..., -0.7302,  0.6408, -0.0522],\n",
       "         ...,\n",
       "         [ 1.2032, -1.3002, -0.9362,  ..., -0.8181, -0.4498,  0.3111],\n",
       "         [ 1.4278, -0.8831, -1.1705,  ..., -1.1583, -0.3076,  0.2316],\n",
       "         [ 1.1652, -1.1560, -1.5016,  ..., -0.4139,  0.0330,  0.0761]],\n",
       "\n",
       "        [[ 0.7961, -0.5861,  1.2442,  ...,  0.4519,  1.5402,  0.2177],\n",
       "         [-0.5975, -1.0323, -0.3382,  ...,  0.0730,  1.2678, -0.9402],\n",
       "         [-1.4447,  0.4205, -0.7397,  ...,  0.5273, -0.9884, -1.6107],\n",
       "         ...,\n",
       "         [ 1.4532, -0.6792, -1.6134,  ..., -1.2216, -0.0779, -0.9343],\n",
       "         [ 0.9494, -1.7667, -0.7953,  ..., -0.9621, -0.1285,  0.3835],\n",
       "         [ 1.1636, -1.9449, -0.6158,  ..., -2.1046,  0.0082,  0.0439]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.4566, -0.4942,  1.0846,  ...,  1.0341,  1.6853,  0.3855],\n",
       "         [-0.2594, -0.4242, -0.3880,  ..., -0.9591,  0.4289, -0.3079],\n",
       "         [ 0.5072, -0.3930,  0.2882,  ...,  0.1148, -0.5321, -1.2522],\n",
       "         ...,\n",
       "         [-0.1653, -1.8457, -1.3196,  ..., -1.4570,  0.1957,  0.9074],\n",
       "         [ 0.6427, -2.2665, -1.6570,  ..., -1.0418,  0.4254, -0.2229],\n",
       "         [ 0.3791, -1.3513, -0.2558,  ..., -0.7671, -0.3185,  0.0213]],\n",
       "\n",
       "        [[ 0.8800,  0.3729,  0.5847,  ...,  0.7384,  0.7147,  0.3269],\n",
       "         [-0.4851, -1.1382, -0.6036,  ..., -0.9577,  1.0344, -0.1901],\n",
       "         [-1.6389,  0.1740, -0.8310,  ..., -0.6244, -0.9123, -0.8923],\n",
       "         ...,\n",
       "         [ 0.7966, -2.2519, -0.7007,  ..., -1.0803, -1.0794, -0.6151],\n",
       "         [ 1.2059, -1.0878, -0.1876,  ..., -0.2924, -0.5723, -0.1752],\n",
       "         [ 0.7114, -0.3376, -1.1219,  ..., -1.1953,  0.1745,  0.1204]],\n",
       "\n",
       "        [[ 0.5595,  0.5921,  0.8107,  ..., -1.3291,  0.4495,  0.4734],\n",
       "         [-1.4448, -0.1222, -0.6498,  ..., -0.0696, -0.6736, -1.1574],\n",
       "         [-0.1506, -0.0345, -0.5092,  ...,  0.4625,  0.6614,  0.3473],\n",
       "         ...,\n",
       "         [ 1.2511, -0.0735, -0.4776,  ..., -0.4544,  0.9914,  0.1213],\n",
       "         [ 0.8498, -1.3520, -0.4374,  ..., -0.2991, -0.5939,  0.3772],\n",
       "         [ 1.3542, -1.5630, -0.4577,  ..., -2.0730,  0.0815, -0.5489]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(itr)\n",
    "res=model(sample['net_input'])\n",
    "#sample['net_input']['src_tokens'].shape, sample['net_input']['src_distance'].shape, sample['net_input']['src_coord'].shape, sample['net_input']['src_edge_type'].shape\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from unicore.data import LMDBDataset\n",
    "#data1=LMDBDataset('/p/scratch/found/structures/EC_test/'+args.train_subset+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data class and testing it later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from unicore.data import LMDBDataset\n",
    "\n",
    "\n",
    "\n",
    "class PocketDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, args, data_dir='/p/scratch/found/EC_test2/',split='train',seq_tokenizer=\"facebook/esm2_t33_650M_UR50D\"):\n",
    "        subset=args.valid_subset.split(\",\")[0]\n",
    "        task=UniMolPocketTask(args, dictionary)\n",
    "        task.load_dataset(subset, combine=False, epoch=1)\n",
    "        self.dataset = task.dataset(subset)\n",
    "        self.h5_file = f'{data_dir}/EC.h5'\n",
    "        meta_file = f'{data_dir}{split}.csv'\n",
    "        print(meta_file,\"meta_file!!!!!!!!!\")\n",
    "        self.meta_data = list(pd.read_csv(meta_file)['name'])\n",
    "        print(len(self.meta_data),\"meta_data!!!!!!!!!\")\n",
    "        \n",
    "        self.seq_tokenizer = AutoTokenizer.from_pretrained(seq_tokenizer)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return idx\n",
    "    \n",
    "    def collate_fn(self, data):\n",
    "        \n",
    "        sequences = []\n",
    "        pockets = []\n",
    "\n",
    "        #seq_ids=data\n",
    "        #print(len(data),\" data in collat!!!!!!!!!\")\n",
    "        \n",
    "        for i in data:\n",
    "            #print(i,\"seq_ids[i]!!!!!!!!!\")\n",
    "            with h5py.File(self.h5_file, 'r') as file:\n",
    "                #print(\"yo open the h5 file!!!!!!!!!\")\n",
    "                for chain in file[f'{self.meta_data[i]}']['structure']['0'].keys():\n",
    "                    sequence = file[f'{self.meta_data[i]}']['structure']['0'][f'{chain}']['residues']['seq1'][()]\n",
    "                    #print(sequence,\"sequence!!!!!!!!!\")\n",
    "                    sequences.append(str(sequence))\n",
    "            data=self.dataset[i]\n",
    "            pocket=dict()\n",
    "            pocket['src_tokens']=data['net_input.src_tokens']\n",
    "            pocket['src_distance']=data['net_input.src_distance']\n",
    "            pocket['src_coord']=data['net_input.src_coord']\n",
    "            pocket['src_edge_type']=data['net_input.src_edge_type']\n",
    "            pockets.append(pocket)\n",
    "        \n",
    "        #print(\"after the loop!!!\")\n",
    "        #print(len(sequences),\"len(sequences)!!!!!!!!!\")\n",
    "        sequence_input = self.seq_tokenizer(sequences, max_length=1024, padding=True, truncation=True, return_tensors=\"pt\").input_ids\n",
    "        #print(sequence_input.shape,\"sequence_input.shape!!!!!!!!!\")\n",
    "        pocket_input={key: torch.stack([d[key] for d in pockets]) for key in pockets[0].keys()}\n",
    "        #print(pocket_input.keys(),\"pocket_input.keys()!!!!!!!!!\")\n",
    "        \n",
    "        return sequence_input.long(), pocket_input\n",
    "        \n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/p/scratch/found/EC_test2/train.csv meta_file!!!!!!!!!\n",
      "13082 meta_data!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "dataset_class=PocketDataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "dataloader=DataLoader(\n",
    "        dataset=dataset_class,\n",
    "        batch_size=32,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        collate_fn=dataset_class.collate_fn,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 32 dict_keys(['src_tokens', 'src_distance', 'src_coord', 'src_edge_type'])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataloader):\n",
    "    print(i,len(data[0]),data[1].keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc_venv_unimol",
   "language": "python",
   "name": "sc_venv_unimol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
